diff --git a/src/cuda/cudaBlend.cu b/src/cuda/cudaBlend.cu
index 4044396..95e3fe7 100644
--- a/src/cuda/cudaBlend.cu
+++ b/src/cuda/cudaBlend.cu
@@ -12,7 +12,9 @@
 #include <cstdio>
 #include <vector>
 
+#ifndef NDEBUG
 #define PRINT_STRANGE_ALPHAS
+#endif
 
 #define EXTRA_ALPHA_CHECKS
 
@@ -380,6 +382,57 @@ __global__ void BatchedComputeLaplacianKernel(
   const T* lowImage = gaussLow + b * lowImageSize;
   T* lapImage = laplacian + b * highImageSize;
 
+  int idxHigh = (y * highWidth + x) * channels;
+
+  if (channels == 4) {
+    const int gxi = x >> 1;
+    const int gyi = y >> 1;
+    const int gxi1 = min(gxi + 1, lowWidth - 1);
+    const int gyi1 = min(gyi + 1, lowHeight - 1);
+
+    const F_T wx1 = (x & 1) ? F_T(0.5) : F_T(0);
+    const F_T wx0 = F_T(1) - wx1;
+    const F_T wy1 = (y & 1) ? F_T(0.5) : F_T(0);
+    const F_T wy0 = F_T(1) - wy1;
+
+    const F_T w00 = wx0 * wy0;
+    const F_T w10 = wx1 * wy0;
+    const F_T w01 = wx0 * wy1;
+    const F_T w11 = wx1 * wy1;
+
+    const int base00 = (gyi * lowWidth + gxi) * channels;
+    const int base10 = (gyi * lowWidth + gxi1) * channels;
+    const int base01 = (gyi1 * lowWidth + gxi) * channels;
+    const int base11 = (gyi1 * lowWidth + gxi1) * channels;
+
+    const bool v00 = static_cast<F_T>(lowImage[base00 + 3]) != F_T(0);
+    const bool v10 = static_cast<F_T>(lowImage[base10 + 3]) != F_T(0);
+    const bool v01 = static_cast<F_T>(lowImage[base01 + 3]) != F_T(0);
+    const bool v11 = static_cast<F_T>(lowImage[base11 + 3]) != F_T(0);
+
+    const F_T w00v = v00 ? w00 : F_T(0);
+    const F_T w10v = v10 ? w10 : F_T(0);
+    const F_T w01v = v01 ? w01 : F_T(0);
+    const F_T w11v = v11 ? w11 : F_T(0);
+
+    const F_T sumW = w00v + w10v + w01v + w11v;
+    const F_T invW = (sumW > F_T(0)) ? (F_T(1) / sumW) : F_T(0);
+
+#pragma unroll
+    for (int c = 0; c < 3; ++c) {
+      const F_T sumV =
+          static_cast<F_T>(lowImage[base00 + c]) * w00v +
+          static_cast<F_T>(lowImage[base10 + c]) * w10v +
+          static_cast<F_T>(lowImage[base01 + c]) * w01v +
+          static_cast<F_T>(lowImage[base11 + c]) * w11v;
+      const F_T upVal = sumV * invW;
+      lapImage[idxHigh + c] = static_cast<T>(static_cast<F_T>(highImage[idxHigh + c]) - upVal);
+    }
+    // alpha channel: just copy high-res alpha
+    lapImage[idxHigh + 3] = highImage[idxHigh + 3];
+    return;
+  }
+
   // map (x,y) in high-res to fractional coord in low-res
   F_T gx = static_cast<F_T>(x) / 2.0f;
   F_T gy = static_cast<F_T>(y) / 2.0f;
@@ -390,8 +443,6 @@ __global__ void BatchedComputeLaplacianKernel(
   int gxi1 = min(gxi + 1, lowWidth - 1);
   int gyi1 = min(gyi + 1, lowHeight - 1);
 
-  int idxHigh = (y * highWidth + x) * channels;
-
   for (int c = 0; c < channels; ++c) {
     if (channels == 4 && c == 3) {
       // alpha channel: just copy high-res alpha
@@ -609,6 +660,66 @@ __global__ void BatchedReconstructKernel(
   const T* lapImage = lap + b * highImageSize;
   T* reconImage = reconstruction + b * highImageSize;
 
+  int idxOut = (y * highWidth + x) * channels;
+
+  if (channels == 4) {
+    const int gxi = x >> 1;
+    const int gyi = y >> 1;
+    const int gxi1 = min(gxi + 1, lowWidth - 1);
+    const int gyi1 = min(gyi + 1, lowHeight - 1);
+
+    const F_T wx1 = (x & 1) ? F_T(0.5) : F_T(0);
+    const F_T wx0 = F_T(1) - wx1;
+    const F_T wy1 = (y & 1) ? F_T(0.5) : F_T(0);
+    const F_T wy0 = F_T(1) - wy1;
+
+    const F_T w00 = wx0 * wy0;
+    const F_T w10 = wx1 * wy0;
+    const F_T w01 = wx0 * wy1;
+    const F_T w11 = wx1 * wy1;
+
+    const int base00 = (gyi * lowWidth + gxi) * channels;
+    const int base10 = (gyi * lowWidth + gxi1) * channels;
+    const int base01 = (gyi1 * lowWidth + gxi) * channels;
+    const int base11 = (gyi1 * lowWidth + gxi1) * channels;
+
+    const bool v00 = static_cast<F_T>(lowImage[base00 + 3]) != F_T(0);
+    const bool v10 = static_cast<F_T>(lowImage[base10 + 3]) != F_T(0);
+    const bool v01 = static_cast<F_T>(lowImage[base01 + 3]) != F_T(0);
+    const bool v11 = static_cast<F_T>(lowImage[base11 + 3]) != F_T(0);
+
+    const F_T w00v = v00 ? w00 : F_T(0);
+    const F_T w10v = v10 ? w10 : F_T(0);
+    const F_T w01v = v01 ? w01 : F_T(0);
+    const F_T w11v = v11 ? w11 : F_T(0);
+
+    const F_T sumW = w00v + w10v + w01v + w11v;
+    const F_T invW = (sumW > F_T(0)) ? (F_T(1) / sumW) : F_T(0);
+
+#pragma unroll
+    for (int c = 0; c < 3; ++c) {
+      const F_T sumV =
+          static_cast<F_T>(lowImage[base00 + c]) * w00v +
+          static_cast<F_T>(lowImage[base10 + c]) * w10v +
+          static_cast<F_T>(lowImage[base01 + c]) * w01v +
+          static_cast<F_T>(lowImage[base11 + c]) * w11v;
+      const F_T upVal = sumV * invW;
+      reconImage[idxOut + c] = static_cast<T>(upVal + static_cast<F_T>(lapImage[idxOut + c]));
+    }
+
+    // Copy alpha channel from Laplacian image
+    const F_T alpha = static_cast<F_T>(lapImage[idxOut + 3]);
+
+#ifdef PRINT_STRANGE_ALPHAS
+    if (alpha != F_T(0) && alpha != F_T(255)) {
+      printf("BatchedReconstructKernel(): Strange alpha %f\n", static_cast<float>(alpha));
+    }
+#endif
+
+    reconImage[idxOut + 3] = static_cast<T>(alpha);
+    return;
+  }
+
   const F_T F_ONE = static_cast<F_T>(1.0);
 
   // No center alignment â€” pure top-left pixel mapping
@@ -623,8 +734,6 @@ __global__ void BatchedReconstructKernel(
   F_T dx = gx - static_cast<F_T>(gxi);
   F_T dy = gy - static_cast<F_T>(gyi);
 
-  int idxOut = (y * highWidth + x) * channels;
-
   for (int c = 0; c < channels; ++c) {
     if (channels == 4 && c == 3) {
       // Copy alpha channel from Laplacian image
@@ -882,16 +991,38 @@ cudaError_t cudaBatchedLaplacianBlendWithContext(
         context.allocation_size += sizeRGB;
         CUDA_CHECK(cudaMalloc((void**)&context.d_gauss2[level], sizeRGB));
         context.allocation_size += sizeRGB;
-      } else {
-        context.d_maskPyr[0] = const_cast<T*>(d_mask);
-        context.d_gauss1[0] = const_cast<T*>(d_image1);
-        context.d_gauss2[0] = const_cast<T*>(d_image2);
       }
     }
   }
 
+  // Level-0 pointers are owned by user code (passed into the function each time).
+  // These may change between calls, even when the context is already initialized.
+  context.d_gauss1[0] = const_cast<T*>(d_image1);
+  context.d_gauss2[0] = const_cast<T*>(d_image2);
+
   dim3 block(16, 16, 1);
 
+  // The blend mask pyramid depends only on the input mask. Build it once and reuse it across
+  // frames to avoid an extra downsample kernel per level, per call.
+  if (context.mask_ptr != d_mask) {
+    context.d_maskPyr[0] = const_cast<T*>(d_mask);
+    context.mask_ptr = d_mask;
+    for (int level = 0; level < context.numLevels - 1; level++) {
+      dim3 gridMask(
+          (context.widths[level + 1] + block.x - 1) / block.x,
+          (context.heights[level + 1] + block.y - 1) / block.y,
+          1);
+      BatchedDownsampleKernelMask<T><<<gridMask, block, 0, stream>>>(
+          context.d_maskPyr[level],
+          context.widths[level],
+          context.heights[level],
+          context.d_maskPyr[level + 1],
+          context.widths[level + 1],
+          context.heights[level + 1]);
+      CUDA_CHECK(cudaGetLastError());
+    }
+  }
+
   // 1. Build Gaussian pyramids.
   for (int level = 0; level < context.numLevels - 1; level++) {
     dim3 grid(
@@ -910,18 +1041,6 @@ cudaError_t cudaBatchedLaplacianBlendWithContext(
         context.batchSize,
         channels);
     CUDA_CHECK(cudaGetLastError());
-    {
-      dim3 gridMask(
-          (context.widths[level + 1] + block.x - 1) / block.x, (context.heights[level + 1] + block.y - 1) / block.y, 1);
-      BatchedDownsampleKernelMask<T><<<gridMask, block, 0, stream>>>(
-          context.d_maskPyr[level],
-          context.widths[level],
-          context.heights[level],
-          context.d_maskPyr[level + 1],
-          context.widths[level + 1],
-          context.heights[level + 1]);
-      CUDA_CHECK(cudaGetLastError());
-    }
   }
 
   // 2. Build Laplacian pyramids.
diff --git a/src/cuda/cudaBlend.h b/src/cuda/cudaBlend.h
index 7a32008..ea3d9b2 100644
--- a/src/cuda/cudaBlend.h
+++ b/src/cuda/cudaBlend.h
@@ -106,6 +106,7 @@ struct CudaBatchLaplacianBlendContext {
   const int imageHeight; ///< Height of the full-resolution image.
   const int batchSize; ///< Number of images in the batch.
   size_t allocation_size{0}; ///< Total allocated device memory size (in bytes).
+  const T* mask_ptr{nullptr}; ///< Base mask pointer used to build d_maskPyr.
 
   std::vector<int> widths; ///< Width of images at each pyramid level.
   std::vector<int> heights; ///< Height of images at each pyramid level.
diff --git a/src/cuda/cudaMakeFull.cu b/src/cuda/cudaMakeFull.cu
index a8065d0..33b2294 100644
--- a/src/cuda/cudaMakeFull.cu
+++ b/src/cuda/cudaMakeFull.cu
@@ -312,6 +312,7 @@ INSTANTIATE_COPY_ROI_BATCHED(half3, uchar3)
 INSTANTIATE_COPY_ROI_BATCHED(half3, uchar4)
 INSTANTIATE_COPY_ROI_BATCHED(float3, float3)
 INSTANTIATE_COPY_ROI_BATCHED(float4, float4)
+INSTANTIATE_COPY_ROI_BATCHED(uchar4, float4)
 INSTANTIATE_COPY_ROI_BATCHED(float3, uchar3)
 INSTANTIATE_COPY_ROI_BATCHED(float4, uchar4)
 INSTANTIATE_COPY_ROI_BATCHED(float4, uchar3)
diff --git a/src/pano/canvasManager.cpp b/src/pano/canvasManager.cpp
index d9d54d8..7593816 100644
--- a/src/pano/canvasManager.cpp
+++ b/src/pano/canvasManager.cpp
@@ -1,5 +1,7 @@
 #include "canvasManager.h"
 
+#include <algorithm>
+
 namespace hm {
 namespace pano {
 
@@ -25,12 +27,13 @@ void CanvasManager::updateMinimizeBlend(const cv::Size& remapped_size_1, const c
 
   // Compute the overlap width based on the first image's width minus the X of the second image.
   int width_1 = _remapper_1.width;
-  _overlapping_width = width_1 - _x2;
-  assert(width_1 > _x2); // Must have positive overlap.
-
-  const int blend_width = _overlapping_width + 2 * _overlap_pad;
+  _overlapping_width = std::max(0, width_1 - _x2);
 
   if (_minimize_blend) {
+    // Minimizing blend requires positive overlap to define the seam strip.
+    assert(width_1 > _x2);
+
+    const int blend_width = _overlapping_width + 2 * _overlap_pad;
     // Assign X offsets for the two remappers for minimal blending region.
     _remapper_1.xpos = _x1;
     // Start overlapping right away in the second remapper.
@@ -57,6 +60,13 @@ void CanvasManager::updateMinimizeBlend(const cv::Size& remapped_size_1, const c
     remapped_image_roi_blend_2 = {0, 0, blend_width - _overlap_pad, remapped_size_2.height};
     assert(remapped_image_roi_blend_1.width == remapped_image_roi_blend_2.width);
     assert(remapped_image_roi_blend_1.x + remapped_image_roi_blend_1.width <= _remapper_1.width);
+  } else {
+    // Full-canvas blending: keep ROIs as the full remapped images placed at their
+    // actual canvas offsets. The seam/mask will not be cropped in this mode.
+    _remapper_1.xpos = _x1;
+    _remapper_2.xpos = _x2;
+    remapped_image_roi_blend_1 = {_x1, 0, remapped_size_1.width, remapped_size_1.height};
+    remapped_image_roi_blend_2 = {_x2, 0, remapped_size_2.width, remapped_size_2.height};
   }
 }
 
diff --git a/src/pano/canvasManager.h b/src/pano/canvasManager.h
index 418f8bf..5e633af 100644
--- a/src/pano/canvasManager.h
+++ b/src/pano/canvasManager.h
@@ -104,6 +104,16 @@ class CanvasManager {
     return _overlap_pad;
   }
 
+  /**
+   * @brief Returns whether blend minimization is enabled.
+   *
+   * When enabled, the seam/mask and blend buffers are cropped to just the
+   * overlapping strip (plus padding) for better performance.
+   */
+  constexpr bool minimize_blend() const {
+    return _minimize_blend;
+  }
+
   /**
    * @brief Returns the overlapping width computed by `updateMinimizeBlend()`.
    */
diff --git a/src/pano/cudaPano.h b/src/pano/cudaPano.h
index 476ef0b..a339ee5 100644
--- a/src/pano/cudaPano.h
+++ b/src/pano/cudaPano.h
@@ -6,6 +6,7 @@
 #include "cupano/pano/controlMasks.h"
 #include "cupano/pano/cudaMat.h"
 
+#include <array>
 #include <memory>
 #include <optional>
 
@@ -38,6 +39,7 @@ struct StitchingContext {
   // Scratch buffers
   std::unique_ptr<CudaMat<T_compute>> cudaFull1;
   std::unique_ptr<CudaMat<T_compute>> cudaFull2;
+  std::unique_ptr<CudaMat<T_compute>> cudaBlended;
 
   // Laplacian Blend Scratch context
   std::unique_ptr<CudaBatchLaplacianBlendContext<BaseScalar_t<T_compute>>> laplacian_blend_context;
@@ -72,7 +74,9 @@ class CudaStitchPano {
       int num_levels,
       const ControlMasks& control_masks,
       bool match_exposure = false,
-      bool quiet = false);
+      bool quiet = false,
+      bool minimize_blend = true,
+      int max_output_width = 0);
 
   int canvas_width() const {
     return canvas_manager_->canvas_width();
@@ -114,6 +118,7 @@ class CudaStitchPano {
   std::unique_ptr<StitchingContext<T_pipeline, T_compute>> stitch_context_;
   std::unique_ptr<CanvasManager> canvas_manager_;
   bool match_exposure_;
+  std::optional<std::array<cv::Point, 2>> exposure_positions_;
   std::optional<float3> image_adjustment_;
   std::optional<cv::Mat> whole_seam_mask_image_;
   CudaStatus status_;
diff --git a/src/pano/cudaPano.inl b/src/pano/cudaPano.inl
index f3f7806..9e18113 100644
--- a/src/pano/cudaPano.inl
+++ b/src/pano/cudaPano.inl
@@ -8,6 +8,8 @@
 #include "cupano/utils/cudaBlendShow.h"
 #include "cupano/utils/showImage.h" /*NOLINT*/
 
+#include <algorithm>
+#include <cmath>
 #include <csignal>
 #include <optional>
 
@@ -21,7 +23,9 @@ CudaStitchPano<T_pipeline, T_compute>::CudaStitchPano(
     int num_levels,
     const ControlMasks& control_masks,
     bool match_exposure,
-    bool quiet)
+    bool quiet,
+    bool minimize_blend,
+    int max_output_width)
     : match_exposure_(match_exposure) {
   if (!control_masks.is_valid()) {
     status_ = CudaStatus(cudaError_t::cudaErrorFileNotFound, "Stitching masks were not able to be loaded");
@@ -29,10 +33,92 @@ CudaStitchPano<T_pipeline, T_compute>::CudaStitchPano(
   }
   stitch_context_ = std::make_unique<StitchingContext<T_pipeline, T_compute>>(
       /*batch_size=*/batch_size, /*is_hard_seam=*/num_levels == 0);
-  assert(!control_masks.positions.empty());
-  // Compute canvas size
-  const int canvas_width = control_masks.canvas_width();
-  const int canvas_height = control_masks.canvas_height();
+  assert(control_masks.positions.size() == 2);
+
+  const int orig_canvas_width = static_cast<int>(control_masks.canvas_width());
+  const int orig_canvas_height = static_cast<int>(control_masks.canvas_height());
+
+  auto pad_mask_to_canvas = [](const cv::Mat& mask, int canvas_w, int canvas_h) -> cv::Mat {
+    int padw = std::max(0, canvas_w - mask.cols);
+    int padh = std::max(0, canvas_h - mask.rows);
+    if (padw > 0 || padh > 0) {
+      cv::Mat padded;
+      cv::copyMakeBorder(mask, padded, 0, padh, 0, padw, cv::BORDER_REPLICATE);
+      assert(padded.cols == canvas_w);
+      assert(padded.rows == canvas_h);
+      return padded;
+    }
+    return mask;
+  };
+
+  // Cache the original seam + positions for exposure matching. These must stay
+  // in the original coordinate system even when we downscale the pano output.
+  cv::Mat seam_mask_padded =
+      pad_mask_to_canvas(control_masks.whole_seam_mask_image, orig_canvas_width, orig_canvas_height);
+  if (match_exposure_) {
+    whole_seam_mask_image_ = seam_mask_padded.clone();
+    exposure_positions_ = std::array<cv::Point, 2>{
+        cv::Point(static_cast<int>(control_masks.positions[0].xpos), static_cast<int>(control_masks.positions[0].ypos)),
+        cv::Point(static_cast<int>(control_masks.positions[1].xpos), static_cast<int>(control_masks.positions[1].ypos)),
+    };
+  }
+
+  // Compute canvas size, optionally downscaling everything up-front so the CUDA
+  // kernels run at the requested output resolution (avoids an expensive post-resize).
+  int canvas_width = orig_canvas_width;
+  int canvas_height = orig_canvas_height;
+  float scale = 1.0f;
+  if (max_output_width > 0 && orig_canvas_width > max_output_width) {
+    canvas_width = max_output_width;
+    if (canvas_width % 2 != 0) {
+      canvas_width -= 1;
+    }
+    canvas_width = std::max(2, canvas_width);
+    scale = static_cast<float>(canvas_width) / static_cast<float>(orig_canvas_width);
+    canvas_height = static_cast<int>(orig_canvas_height * scale);
+    if (canvas_height % 2 != 0) {
+      canvas_height -= 1;
+    }
+    canvas_height = std::max(2, canvas_height);
+  }
+
+  // Locals that may be resized when scale != 1.0
+  cv::Mat img1_col = control_masks.img1_col;
+  cv::Mat img1_row = control_masks.img1_row;
+  cv::Mat img2_col = control_masks.img2_col;
+  cv::Mat img2_row = control_masks.img2_row;
+  cv::Mat seam_mask = seam_mask_padded;
+
+  std::array<cv::Point, 2> canvas_positions{
+      cv::Point(static_cast<int>(control_masks.positions[0].xpos), static_cast<int>(control_masks.positions[0].ypos)),
+      cv::Point(static_cast<int>(control_masks.positions[1].xpos), static_cast<int>(control_masks.positions[1].ypos)),
+  };
+
+  if (scale != 1.0f) {
+    auto scale_point = [&](const SpatialTiff& pos) -> cv::Point {
+      return cv::Point(static_cast<int>(pos.xpos * scale), static_cast<int>(pos.ypos * scale));
+    };
+    canvas_positions = {scale_point(control_masks.positions[0]), scale_point(control_masks.positions[1])};
+
+    auto clamp_resize = [&](const cv::Mat& src, const cv::Point& pos) -> cv::Size {
+      int w = std::max(1, static_cast<int>(src.cols * scale));
+      int h = std::max(1, static_cast<int>(src.rows * scale));
+      int max_w = std::max(1, canvas_width - pos.x);
+      int max_h = std::max(1, canvas_height - pos.y);
+      w = std::min(w, max_w);
+      h = std::min(h, max_h);
+      return cv::Size(w, h);
+    };
+
+    const cv::Size s1 = clamp_resize(img1_col, canvas_positions[0]);
+    const cv::Size s2 = clamp_resize(img2_col, canvas_positions[1]);
+
+    cv::resize(img1_col, img1_col, s1, 0, 0, cv::INTER_NEAREST);
+    cv::resize(img1_row, img1_row, s1, 0, 0, cv::INTER_NEAREST);
+    cv::resize(img2_col, img2_col, s2, 0, 0, cv::INTER_NEAREST);
+    cv::resize(img2_row, img2_row, s2, 0, 0, cv::INTER_NEAREST);
+    cv::resize(seam_mask, seam_mask, cv::Size(canvas_width, canvas_height), 0, 0, cv::INTER_NEAREST);
+  }
 
   if (!quiet) {
     std::cout << "Stitched canvas size: " << canvas_width << " x " << canvas_height << std::endl;
@@ -41,35 +127,29 @@ CudaStitchPano<T_pipeline, T_compute>::CudaStitchPano(
   //
   // CanvasManager
   //
+  const bool enable_minimize_blend = minimize_blend && !stitch_context_->is_hard_seam();
   canvas_manager_ = std::make_unique<CanvasManager>(
       CanvasInfo{
-          .width = canvas_width,
-          .height = canvas_height,
-          .positions =
-              {cv::Point(control_masks.positions[0].xpos, control_masks.positions[0].ypos),
-               cv::Point(control_masks.positions[1].xpos, control_masks.positions[1].ypos)}},
-      /*minimize_blend=*/!stitch_context_->is_hard_seam());
+          .width = canvas_width, .height = canvas_height, .positions = {canvas_positions[0], canvas_positions[1]}},
+      /*minimize_blend=*/enable_minimize_blend);
 
-  canvas_manager_->_remapper_1.width = control_masks.img1_col.cols;
-  canvas_manager_->_remapper_1.height = control_masks.img1_col.rows;
-  canvas_manager_->_remapper_2.width = control_masks.img2_col.cols;
-  canvas_manager_->_remapper_2.height = control_masks.img2_col.rows;
+  canvas_manager_->_remapper_1.width = img1_col.cols;
+  canvas_manager_->_remapper_1.height = img1_col.rows;
+  canvas_manager_->_remapper_2.width = img2_col.cols;
+  canvas_manager_->_remapper_2.height = img2_col.rows;
 
-  canvas_manager_->updateMinimizeBlend(control_masks.img1_col.size(), control_masks.img2_col.size());
+  canvas_manager_->updateMinimizeBlend(img1_col.size(), img2_col.size());
 
-  cv::Mat blend_seam = canvas_manager_->convertMaskMat(control_masks.whole_seam_mask_image);
+  cv::Mat blend_seam = canvas_manager_->convertMaskMat(seam_mask);
   assert(!blend_seam.empty());
   blend_seam = blend_seam.clone();
 
-  auto canvas = std::make_unique<CudaMat<T_pipeline>>(
-      stitch_context_->batch_size(), canvas_manager_->canvas_width(), canvas_manager_->canvas_height());
-
-  assert(control_masks.img1_col.type() == CV_16U);
-  stitch_context_->remap_1_x = std::make_unique<CudaMat<uint16_t>>(control_masks.img1_col);
-  stitch_context_->remap_1_y = std::make_unique<CudaMat<uint16_t>>(control_masks.img1_row);
+  assert(img1_col.type() == CV_16U);
+  stitch_context_->remap_1_x = std::make_unique<CudaMat<uint16_t>>(img1_col);
+  stitch_context_->remap_1_y = std::make_unique<CudaMat<uint16_t>>(img1_row);
 
-  stitch_context_->remap_2_x = std::make_unique<CudaMat<uint16_t>>(control_masks.img2_col);
-  stitch_context_->remap_2_y = std::make_unique<CudaMat<uint16_t>>(control_masks.img2_row);
+  stitch_context_->remap_2_x = std::make_unique<CudaMat<uint16_t>>(img2_col);
+  stitch_context_->remap_2_y = std::make_unique<CudaMat<uint16_t>>(img2_row);
 
   if (!stitch_context_->is_hard_seam()) {
     blend_seam.convertTo(blend_seam, cudaPixelTypeToCvType(CudaTypeToPixelType<T_compute>::value));
@@ -77,6 +157,8 @@ CudaStitchPano<T_pipeline, T_compute>::CudaStitchPano(
         std::make_unique<CudaMat<T_compute>>(stitch_context_->batch_size(), blend_seam.cols, blend_seam.rows);
     stitch_context_->cudaFull2 =
         std::make_unique<CudaMat<T_compute>>(stitch_context_->batch_size(), blend_seam.cols, blend_seam.rows);
+    stitch_context_->cudaBlended =
+        std::make_unique<CudaMat<T_compute>>(stitch_context_->batch_size(), blend_seam.cols, blend_seam.rows);
 
     stitch_context_->cudaBlendSoftSeam = std::make_unique<CudaMat<T_compute>>(blend_seam);
     stitch_context_->laplacian_blend_context =
@@ -85,13 +167,29 @@ CudaStitchPano<T_pipeline, T_compute>::CudaStitchPano(
             stitch_context_->cudaBlendSoftSeam->height(),
             num_levels,
             /*batch_size=*/stitch_context_->batch_size());
+
+    // One-time clear: the per-frame ROI copies intentionally do not touch the padding/empty
+    // regions of these full-frame blend buffers.
+    const size_t full1_bytes = static_cast<size_t>(stitch_context_->cudaFull1->pitch()) *
+        static_cast<size_t>(stitch_context_->cudaFull1->height()) *
+        static_cast<size_t>(stitch_context_->cudaFull1->batch_size());
+    cudaError_t clear_err = cudaMemset(stitch_context_->cudaFull1->data(), 0, full1_bytes);
+    if (clear_err != cudaSuccess) {
+      status_ = CudaStatus(clear_err, "Failed to clear blend buffer cudaFull1");
+      return;
+    }
+    const size_t full2_bytes = static_cast<size_t>(stitch_context_->cudaFull2->pitch()) *
+        static_cast<size_t>(stitch_context_->cudaFull2->height()) *
+        static_cast<size_t>(stitch_context_->cudaFull2->batch_size());
+    clear_err = cudaMemset(stitch_context_->cudaFull2->data(), 0, full2_bytes);
+    if (clear_err != cudaSuccess) {
+      status_ = CudaStatus(clear_err, "Failed to clear blend buffer cudaFull2");
+      return;
+    }
   } else {
     assert(blend_seam.type() == CV_8U);
     stitch_context_->cudaBlendHardSeam = std::make_unique<CudaMat<unsigned char>>(blend_seam);
   }
-  if (match_exposure_) {
-    whole_seam_mask_image_ = control_masks.whole_seam_mask_image;
-  }
 }
 
 namespace tmp {
@@ -129,8 +227,28 @@ CudaStatusOr<std::unique_ptr<CudaMat<T_pipeline>>> CudaStitchPano<T_pipeline, T_
   const size_t batch_size = stitch_context.batch_size();
 
   assert(canvas->pitch());
-  CUDA_RETURN_IF_ERROR(
-      cudaMemsetAsync(canvas->data_raw(), 0, canvas->pitch() * canvas->height() * stitch_context.batch_size(), stream));
+  bool clear_canvas = true;
+  if (!stitch_context.is_hard_seam()) {
+    // If we're doing a full-canvas blend, the final ROI copy overwrites every pixel.
+    if (!canvas_manager.minimize_blend()) {
+      clear_canvas = false;
+    } else {
+      // When the two remap rectangles fully cover the output canvas, every pixel is written
+      // by at least one remap kernel (including unmapped pixels via the default fill).
+      const bool covers_full_height = (canvas_manager._y1 == 0 && canvas_manager._y2 == 0 &&
+          canvas_manager._remapper_1.height == canvas->height() && canvas_manager._remapper_2.height == canvas->height());
+      const bool covers_full_width = (canvas_manager._x1 == 0 && canvas_manager._x2 <= canvas_manager._remapper_1.width &&
+          (canvas_manager._x2 + canvas_manager._remapper_2.width) >= canvas->width());
+      clear_canvas = !(covers_full_height && covers_full_width);
+    }
+  }
+  if (clear_canvas) {
+    CUDA_RETURN_IF_ERROR(cudaMemsetAsync(
+        canvas->data_raw(),
+        0,
+        canvas->pitch() * canvas->height() * stitch_context.batch_size(),
+        stream));
+  }
 
   // TODO: remove me
   // CUDA_RETURN_IF_ERROR(cudaMemsetAsync(stitch_context.cudaFull1->data_raw(), 0, stitch_context.cudaFull1->pitch() *
@@ -193,18 +311,16 @@ CudaStatusOr<std::unique_ptr<CudaMat<T_pipeline>>> CudaStitchPano<T_pipeline, T_
     //
     // Now copy the blending portion of remapped image 1 from the canvas onto the blend image
     //
-    cuerr = simple_make_full_batch(
-        // Image 1 (float image)
+    cuerr = copy_roi_batched(
         canvas->surface(),
-        /*region_width=*/roi_width(canvas_manager.remapped_image_roi_blend_1),
-        /*region_height=*/stitch_context.cudaBlendSoftSeam->height(),
-        canvas_manager.remapped_image_roi_blend_1.x,
-        0 /* we've already applied our Y offset */,
-        /*destOffsetX=*/canvas_manager._remapper_1.xpos,
-        /*destOffsetY=*/0,
-        /*adjust_origin=*/false,
-        /*batchSize=*/batch_size,
+        /*regionWidth=*/roi_width(canvas_manager.remapped_image_roi_blend_1),
+        /*regionHeight=*/stitch_context.cudaBlendSoftSeam->height(),
+        /*srcROI_x=*/canvas_manager.remapped_image_roi_blend_1.x,
+        /*srcROI_y=*/0 /* we've already applied our Y offset */,
         stitch_context.cudaFull1->surface(),
+        /*offsetX=*/canvas_manager._remapper_1.xpos,
+        /*offsetY=*/0,
+        /*batchSize=*/batch_size,
         stream);
     CUDA_RETURN_IF_ERROR(cuerr);
     // SHOW_SCALED_BATCH_ITEM(stitch_context.cudaFull1, 0.2, 0);
@@ -307,18 +423,16 @@ CudaStatusOr<std::unique_ptr<CudaMat<T_pipeline>>> CudaStitchPano<T_pipeline, T_
     //
     // Now copy the blending portion of remapped image 2 from the canvas onto the blend image
     //
-    cuerr = simple_make_full_batch(
-        // Image 1 (float image)
+    cuerr = copy_roi_batched(
         canvas->surface(),
-        /*region_width=*/roi_width(canvas_manager.remapped_image_roi_blend_2),
-        /*region_height=*/stitch_context.cudaBlendSoftSeam->height() /*roi_height(canvas_manager.roi_blend_2)*/,
-        /*offsetX=*/canvas_manager._x2,
-        /*offsetY=*/0, // we've already applied the Y offset when painting it onto the canvas
-        /*destOffsetX=*/canvas_manager._remapper_2.xpos,
-        /*destOffsetY=*/0 /* we've already applied our Y offset */,
-        /*adjust_origin=*/false,
-        /*batchSize=*/stitch_context.batch_size(),
+        /*regionWidth=*/roi_width(canvas_manager.remapped_image_roi_blend_2),
+        /*regionHeight=*/stitch_context.cudaBlendSoftSeam->height(),
+        /*srcROI_x=*/canvas_manager._x2,
+        /*srcROI_y=*/0, // we've already applied the Y offset when painting it onto the canvas
         stitch_context.cudaFull2->surface(),
+        /*offsetX=*/canvas_manager._remapper_2.xpos,
+        /*offsetY=*/0,
+        /*batchSize=*/stitch_context.batch_size(),
         stream);
     CUDA_RETURN_IF_ERROR(cuerr);
     // SHOW_SCALED(stitch_context.cudaFull2, 0.5);
@@ -370,7 +484,7 @@ CudaStatusOr<std::unique_ptr<CudaMat<T_pipeline>>> CudaStitchPano<T_pipeline, T_
 #endif
   }
   if (!stitch_context.is_hard_seam()) {
-    CudaMat<T_compute>& cudaBlendedFull = *stitch_context.cudaFull1;
+    CudaMat<T_compute>& cudaBlendedFull = *stitch_context.cudaBlended;
 #if 0
     if constexpr (sizeof(T_compute) / sizeof(BaseScalar_t<T_compute>) == 4) {
       auto surf1 = stitch_context.cudaFull1->surface();
@@ -392,7 +506,6 @@ CudaStatusOr<std::unique_ptr<CudaMat<T_pipeline>>> CudaStitchPano<T_pipeline, T_
         stitch_context.cudaFull1->data_raw(),
         stitch_context.cudaFull2->data_raw(),
         stitch_context.cudaBlendSoftSeam->data_raw(),
-        // Put output in full-1 memory
         cudaBlendedFull.data_raw(),
         *stitch_context.laplacian_blend_context,
         stitch_context.cudaFull2->channels(),
@@ -405,6 +518,10 @@ CudaStatusOr<std::unique_ptr<CudaMat<T_pipeline>>> CudaStitchPano<T_pipeline, T_
     // Copy the blended portion (overlapping portion + some padding) onto
     // the canvas over some of the remapped image 1 and image 2
     //
+    int out_offset_x = 0;
+    if (canvas_manager.minimize_blend()) {
+      out_offset_x = canvas_manager._x2 - canvas_manager.overlap_padding();
+    }
     cuerr = copy_roi_batched(
         cudaBlendedFull.surface(),
         cudaBlendedFull.width(),
@@ -412,7 +529,7 @@ CudaStatusOr<std::unique_ptr<CudaMat<T_pipeline>>> CudaStitchPano<T_pipeline, T_
         0,
         0,
         canvas->surface(),
-        /*offsetX=*/canvas_manager._x2 - canvas_manager.overlap_padding(),
+        /*offsetX=*/out_offset_x,
         /*offsetY=*/0,
         /*batchSize=*/stitch_context.batch_size(),
         stream);
@@ -430,13 +547,19 @@ std::optional<float3> CudaStitchPano<T_pipeline, T_compute>::compute_image_adjus
     const CudaMat<T_pipeline>& inputImage2) {
   cv::Mat tmp1 = inputImage1.download();
   cv::Mat tmp2 = inputImage2.download();
+  cv::Point top_left_1 = canvas_manager_->canvas_positions()[0];
+  cv::Point top_left_2 = canvas_manager_->canvas_positions()[1];
+  if (exposure_positions_.has_value()) {
+    top_left_1 = (*exposure_positions_)[0];
+    top_left_2 = (*exposure_positions_)[1];
+  }
   std::optional<cv::Scalar> adjustment_result = match_seam_images(
       tmp1,
       tmp2,
       *whole_seam_mask_image_,
       /*N=*/100,
-      canvas_manager_->canvas_positions()[0],
-      canvas_manager_->canvas_positions()[1]);
+      top_left_1,
+      top_left_2);
   if (adjustment_result.has_value()) {
     const cv::Scalar& adjustment = *adjustment_result;
     return float3{
