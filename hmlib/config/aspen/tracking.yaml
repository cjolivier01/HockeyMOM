aspen:
  minimal_context: true

  # Move the original mmengine inference pipeline here so the CLI can use it
  inference_pipeline:
    - type: LoadImageFromFile
    - type: HmCrop
      keys: [img]
      save_clipped_images: true
    - type: HmImageToTensor
      keys: [img]
      # scale_factor: 255.0
    - type: HmResize
      # We will scale downm the image to fit within img_scale
      # before HmPad maybe pads it to be this size
      # img_scale: [1312, 480]
      img_scale: [1632, 608]
      # img_scale: [1952, 704]
      # img_scale: [1984, 736]
      keep_ratio: true
      interpolation: area
    - type: HmPad
      pad_val: 114.0
      # size_divisor: 32
      # size: [736, 1984]  # Actual size of tensor, which is H-W ordering
      # size: [480, 1312]  # Actual size of tensor, which is H-W ordering
      size: [608, 1632]  # Actual size of tensor, which is H-W ordering
    - type: mmdet.PackTrackInputs
      meta_keys: [img_metas]

  # Default camera post-processing pipeline (was in hm_end_to_end.py)
  video_out_pipeline:
    - type: HmConfigureScoreboard
    - type: HmCaptureScoreboard
    - type: HmPerspectiveRotation
      pre_clip: true
    - type: HmCropToVideoFrame
    - type: HmRenderScoreboard
      image_labels: [img, end_zone_img]
    - type: HmUnsharpMask
      enabled: false
      image_label: img
    - type: HmImageOverlays
      watermark_config:
        image: images/sports_ai_watermark.png

  trunks:
    image_prep:
      class: hmlib.aspen.trunks.image_prep.ImagePrepTrunk
      depends: []
      params: {}

    model_factory:
      class: hmlib.aspen.trunks.model_factory.ModelFactoryTrunk
      depends: []
      params:
        model_class: hmlib.models.end_to_end.HmEndToEnd
        # Pure YAML detector definition (converted from mmengine config)
        # No detector/tracker coupling here; post_* pipelines not used for rink mask
        post_tracking_pipeline:
          - type: HmNumberClassifier
            image_label: original_images
            enabled: false

    detector_factory:
      class: hmlib.aspen.trunks.detector_factory.DetectorFactoryTrunk
      depends: []
      params:
        detector_yaml: hmlib/config/aspen/models/bytetrack_yolox_s.detector.yaml
        # detector_yaml: hmlib/config/aspen/models/hm_crowdhuman_yolov8_m_1984_736.detector.yaml
        data_preprocessor:
          type: TrackDataPreprocessor
          pad_size_divisor: 32
          use_det_processor: true
          batch_augments:
            - type: BatchSyncRandomResize
              random_size_range: [576, 1024]
              size_divisor: 32
              interval: 10

    boundaries:
      class: hmlib.aspen.trunks.boundaries.BoundariesTrunk
      depends: [model_factory]
      params: {}

    detector:
      class: hmlib.aspen.trunks.detector.DetectorInferenceTrunk
      depends: [image_prep, detector_factory]
      params: {}

    ice_config:
      class: hmlib.aspen.trunks.ice_rink_boundaries.IceRinkSegmConfigTrunk
      depends: [image_prep]
      params: {}

    ice_boundaries:
      class: hmlib.aspen.trunks.ice_rink_boundaries.IceRinkSegmBoundariesTrunk
      depends: [detector, ice_config]
      params:
        enabled: true

    tracker:
      class: hmlib.aspen.trunks.tracker.TrackerTrunk
      depends: [detector, ice_boundaries, model_factory, boundaries]
      params: {}

    # Optional camera controller (rule-based by default). Enable transformer by
    # setting controller: transformer and model_path to your checkpoint.
    camera_controller:
      class: hmlib.aspen.trunks.camera_controller.CameraControllerTrunk
      depends: [tracker]
      params:
        controller: rule
        model_path: null
        window: 8

    postprocess:
      class: hmlib.aspen.trunks.postprocess.CamPostProcessTrunk
      depends: [tracker, camera_controller]
      params: {}
